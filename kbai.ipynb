{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kbai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VzKxpCAFbCkriluqdpJJZjTqs95dFOtz",
      "authorship_tag": "ABX9TyNdIvdgpbXoqsgI6P7f7kGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fllay/kbai_notebook/blob/main/kbai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmR-v3El7OAG"
      },
      "source": [
        "%tensorflow_version 1.x  #Select module of the tensorflow\n",
        "!pip show tensorflow\n",
        "!pip install tf_slim\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTOmRra7M80K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaKgmzwm7bay"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AID2mVJa2RXO"
      },
      "source": [
        "import io\n",
        "from PIL import Image\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU4HZ7H5piBL"
      },
      "source": [
        "MODEL_NAME = \"V1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEL8q_mOtlpo"
      },
      "source": [
        "%cd /root/\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/root/models/research/:/root/models/research/slim/:/root/models/research/object_detection/utils/:/root/models/research/object_detection'\n",
        "!git clone --quiet https://github.com/PINTO0309/TPU-MobilenetSSD.git\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "!apt-get install -qq protobuf-compiler python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive\n",
        "!pip install -q pycocotools\n",
        "!pip install tf_slim\n",
        "%cd ~/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "%cd\n",
        "!git clone https://github.com/cocodataset/cocoapi.git\n",
        "%cd cocoapi/PythonAPI\n",
        "!make\n",
        "!cp -r pycocotools /root/models/research/\n",
        "\n",
        "%cd /root/models/research\n",
        "!python setup.py build"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7SfuprGsTQr"
      },
      "source": [
        "os.environ['PYTHONPATH'] += ':/root/models/research/:/root/models/research/slim/:/root/models/research/object_detection/utils/:/root/models/research/object_detection'\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from google.protobuf import text_format\n",
        "from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n",
        "from object_detection.protos import pipeline_pb2\n",
        "from object_detection.utils import dataset_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sxV_yPOczzO"
      },
      "source": [
        "%cd /content/\n",
        "!wget https://github.com/mmatczuk/go-http-tunnel/releases/download/2.1/tunnel_linux_amd64.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoeorQ-0c-tl"
      },
      "source": [
        "#!tar -xvf tunnel_linux_amd64.tar.gz\n",
        "!git clone https://github.com/fllay/kidbrightTunnel.git \n",
        "!cp /content/kidbrightTunnel/client.* .\n",
        "!cp /content/kidbrightTunnel/tunnel .\n",
        "!chmod 755 tunnel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFEkQqevfKaU"
      },
      "source": [
        "#!ls -al\n",
        "#!/content/tunnel  -config /content/tunnel.yml  start-all "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feS8TI6zAJ_K"
      },
      "source": [
        "import random\n",
        "portnum = str(random.randint(1000, 2000))\n",
        "SSHPORT = '0.0.0.0:' + str(portnum)\n",
        "ADDRESS = 'web' + str(portnum) + '.tunel.imsai.us' \n",
        "print(SSHPORT)\n",
        "print(ADDRESS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44UA5tejeFRV"
      },
      "source": [
        "import yaml\n",
        "data = dict(\n",
        "    server_addr = '203.154.83.237:5223',\n",
        "    tunnels = dict(\n",
        "        ssh = dict(\n",
        "          proto = 'tcp',\n",
        "          addr = 'localhost:22',\n",
        "          remote_addr = SSHPORT,\n",
        "          \n",
        "        ),\n",
        "        webui = dict(\n",
        "          proto = 'http',\n",
        "          addr = 'http://localhost:5000',\n",
        "          host = ADDRESS,\n",
        "        )\n",
        "        \n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "ADDRESS2 = 'web' + str(portnum) + '.tunnel.kid-bright.org' \n",
        "data2 = dict(\n",
        "    server_addr = '203.154.39.67:5223',\n",
        "    tunnels = dict(\n",
        "        ssh = dict(\n",
        "          proto = 'tcp',\n",
        "          addr = 'localhost:22',\n",
        "          remote_addr = SSHPORT,\n",
        "          \n",
        "        ),\n",
        "        webui = dict(\n",
        "          proto = 'http',\n",
        "          addr = 'http://localhost:5000',\n",
        "          host = ADDRESS2,\n",
        "        )\n",
        "        \n",
        "    )\n",
        ")\n",
        "\n",
        "with open('tunnel.yml', 'w') as outfile:\n",
        "    yaml.dump(data, outfile, default_flow_style=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku_EZuowgL-N"
      },
      "source": [
        "!cat ./tunnel.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDJ68-UIeTCy"
      },
      "source": [
        "!openssl req -x509 -nodes -newkey rsa:2048 -sha256 -keyout client.key -out client.crt -subj \"/C=GB/ST=London/L=London/O=Global Security/OU=IT Department/CN=example.com\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slw9zQHEdDPE"
      },
      "source": [
        "!ls -l\n",
        "!pwd\n",
        "#!/content/tunnel  -config tunnel.yml start-all \n",
        "import os\n",
        "os.system('/content/tunnel  -config /content/tunnel.yml  start-all  &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKXiweOzdVky"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mahGKfzItkda"
      },
      "source": [
        "!npm i -g npm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pheOK-7glX"
      },
      "source": [
        "!npm install -g @vue/cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSyFPLun8JMz"
      },
      "source": [
        "!pip install flask-ngrok xmltodict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRE9CFdsScIk"
      },
      "source": [
        "#!git clone https://github.com/fllay/kbai.git \n",
        "!git clone https://github.com/ThundluckS/kbai.git\n",
        "#!git clone https://fllay@bitbucket.org/fllay/kbai.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEmNMh7_90DZ"
      },
      "source": [
        "%cd /content/kbai\n",
        "!ls -al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8uAOzep9-Nw"
      },
      "source": [
        "!npm install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN9HCp9va6ex"
      },
      "source": [
        "!npm run build"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2g84HZWjWOE"
      },
      "source": [
        "!ls -l dist/static\n",
        "%cd dist/static\n",
        "!git clone https://github.com/fllay/ruri.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlzK2-E5JKbk"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJgM3RrGms1u"
      },
      "source": [
        "%cd /content/kbai/dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQoT2RfsfOiw"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        #print(xml_file)\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            \n",
        "            try:\n",
        "              #print(member[0].text)\n",
        "              value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text)\n",
        "                     )\n",
        "              xml_list.append(value)\n",
        "            except IndexError:\n",
        "              pass\n",
        "    \n",
        "            \n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path, label_dict):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = [] \n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(label_dict[row['class']])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def gen_tfrecord(image_dir, csv_input, output_path, label_dict):\n",
        "    writer = tf.python_io.TFRecordWriter(output_path)\n",
        "    path = image_dir #os.path.join(image_dir)\n",
        "    examples = pd.read_csv(csv_input)\n",
        "    #print(examples)\n",
        "    grouped = split(examples, 'filename')\n",
        "    #print(grouped)\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path, label_dict)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cizrIAFg4MYg"
      },
      "source": [
        "import urllib\n",
        "import tarfile\n",
        "from requests import get\n",
        "\n",
        "\n",
        "#ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
        "\n",
        "if(MODEL_NAME == \"V1\"):\n",
        "  MODEL = 'ssd_mobilenet_v1_coco_2018_01_28'\n",
        "else:  \n",
        "  MODEL = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
        "#http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz\n",
        "\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = 'pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "  with open(MODEL_FILE, \"wb\") as file:\n",
        "    # get request\n",
        "    response = get(DOWNLOAD_BASE + MODEL_FILE)\n",
        "    # write to file\n",
        "    file.write(response.content)\n",
        "    #opener = urllib.URLopener()\n",
        "    #opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\n",
        "    for item in os.listdir(src):\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks, ignore)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict\n",
        "\n",
        "def list_paths(path):\n",
        "  directories = [x[1] for x in os.walk(path)]\n",
        "  non_empty_dirs = [x for x in directories if x] # filter out empty lists\n",
        "  return [item for subitem in non_empty_dirs for item in subitem] # flatten the list\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFPFcD0rE4vn"
      },
      "source": [
        "!ls -l /root/models/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VanE48L87Bvv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m6zd32lTcyz"
      },
      "source": [
        "!pip install cryptography\n",
        "!pip install paramiko\n",
        "!pip install pyopenssl\n",
        "%cd /content/kbai\n",
        "!openssl req -x509 -newkey rsa:4096 -nodes -out cert.pem -keyout key.pem -days 365 -subj \"/C=GB/ST=London/L=London/O=Global Security/OU=IT Department/CN=example.com\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoAc5CSv8O46"
      },
      "source": [
        "NUM_OF_TRAINING_STEP = 1000\n",
        "#from flask_ngrok import run_with_ngrok\n",
        "import shutil\n",
        "from flask import Flask, render_template, request, abort, jsonify, send_file\n",
        "import os\n",
        "import glob\n",
        "from xml.etree import ElementTree as ET\n",
        "import xmltodict\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "from base64 import encodebytes\n",
        "from io import BytesIO\n",
        "import subprocess\n",
        "\n",
        "\n",
        "gl = []\n",
        "is_done = True\n",
        "\n",
        "def run(cmd):\n",
        "  global gl\n",
        "  output = get_ipython().getoutput(cmd)\n",
        "  print(output)\n",
        "  gl = gl + output\n",
        "\n",
        "#TEMPLATE_DIR = os.path.abspath('./templates')\n",
        "#STATIC_DIR = os.path.abspath('./static')\n",
        "TEMPLATE_DIR = os.path.abspath('./dist')\n",
        "STATIC_DIR = os.path.abspath('./dist/static')\n",
        "GOOGLE_DRIVE = os.path.abspath('/content/drive/My\\ Drive/')\n",
        "print(TEMPLATE_DIR)\n",
        "print(STATIC_DIR)\n",
        "detection_graph = tf.Graph()\n",
        "app = Flask(__name__, template_folder=TEMPLATE_DIR, static_folder=STATIC_DIR) # template_folder='/Users/cake/Desktop/Projects/kidbrightAI/testVue/kidbrightai/dist')\n",
        "#run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/getDrive', methods=['GET'])\n",
        "def getDrive():\n",
        "    drive = {\n",
        "                  'drives': 'OK',\n",
        "          'mountPoints': TEMPLATE_DIR\n",
        "    }\n",
        "    return jsonify(drive), 201\n",
        "\n",
        "\n",
        "@app.route('/createProject', methods=['POST'])\n",
        "def createProject():\n",
        "    #data = request.form\n",
        "    STATIC_DIR\n",
        "    content = request.json\n",
        "    print(content[\"projectDir\"])\n",
        "    try:\n",
        "        os.mkdir(os.path.join(STATIC_DIR, content[\"projectDir\"]))\n",
        "        os.mkdir(os.path.join(STATIC_DIR, content[\"projectDir\"], \"images\"))\n",
        "        os.mkdir(os.path.join(STATIC_DIR, content[\"projectDir\"], \"imgclass\"))\n",
        "        f = open(os.path.join(STATIC_DIR, content[\"projectDir\"], \"imclass.json\"), \"a\")\n",
        "        f.write(\"{}\")\n",
        "        f.close()\n",
        "        fxml = open(os.path.join(STATIC_DIR, content[\"projectDir\"], \"project.xml\"), \"a\")\n",
        "        fxml.write(\" \")\n",
        "        fxml.close()\n",
        "        res = {'status': 'OK', 'mountPoints': content[\"projectDir\"]}\n",
        "    except OSError:\n",
        "        print (\"Creation of the directory  failed\")\n",
        "        res = {\n",
        "            'status': 'not OK',\n",
        "            'mountPoints': TEMPLATE_DIR\n",
        "        }\n",
        "  \n",
        "        return jsonify(res), 201\n",
        "    else:\n",
        "        print (\"Successfully created the directory\")\n",
        "        res = {\n",
        "            'status': 'OK',\n",
        "            'mountPoints': TEMPLATE_DIR\n",
        "        }\n",
        "  \n",
        "        return jsonify(res), 201\n",
        "\n",
        "\n",
        "@app.route('/getFiles', methods=['POST'])\n",
        "def getFiles():\n",
        "    allfiles = []\n",
        "    rjson = request.json\n",
        "    projectName = rjson[\"path\"]\n",
        "    for file in glob.glob(\"./dist/static/\" + projectName +\"/images/*.png\"):\n",
        "        \n",
        "                \n",
        "        #(os.path.relpath(file, 'dist'))\n",
        "\n",
        "        allfiles.append({'file': os.path.relpath(file, 'dist'),\n",
        "            'id': 1,\n",
        "            'isAnotated': False,\n",
        "            'class': 'totoro',\n",
        "            'classCounts': 5})\n",
        "\n",
        "        \n",
        "    \n",
        "    print(allfiles)\n",
        "\n",
        "    res = {\n",
        "        'status': 'OK',\n",
        "        'mountPoints': TEMPLATE_DIR,\n",
        "        'files': allfiles\n",
        "    }\n",
        "  \n",
        "    return jsonify(res), 201\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/getProjects', methods=['GET'])\n",
        "def getProjects():\n",
        "    dds = []\n",
        "    for x in os.listdir('./dist/static'):\n",
        "        for fname in os.listdir('./dist/static/' + x):\n",
        "            if fname.endswith('.xml'):\n",
        "                # do stuff on the file\n",
        "                print(x)\n",
        "                dds.append(x)\n",
        "                break\n",
        "            else:\n",
        "            # do stuff if a file .true doesn't exist.\n",
        "                pass\n",
        "            \n",
        "    res = {\n",
        "        'status': 'OK',\n",
        "        'projects': dds\n",
        "    }\n",
        "  \n",
        "    return jsonify(res), 201\n",
        "\n",
        "@app.route('/importFromGoogleDrive', methods=[\"POST\"])\n",
        "def importFromGoogleDrive():\n",
        "  rjson = request.json\n",
        "  project_dir = os.path.join(STATIC_DIR, rjson[\"projectName\"])\n",
        "  gs_project_dir = os.path.join('/content/drive/My Drive/', rjson[\"projectName\"])\n",
        "  print(rjson[\"projectName\"])\n",
        "  print(gs_project_dir)\n",
        "  print('cp -rf ' + gs_project_dir + ' ' + STATIC_DIR)\n",
        "  print(os.path.isdir(gs_project_dir))\n",
        "  if(os.path.isdir(project_dir) == True):\n",
        "    print(\"directory exists\")\n",
        "    os.system('rm -rf ' + project_dir)\n",
        "    #os.system('cp -rf ' + gs_project_dir + ' ' + STATIC_DIR)\n",
        "\n",
        "    copytree(gs_project_dir, project_dir)\n",
        "  else:\n",
        "    #os.system('cp -rf ' + gs_project_dir + ' ' + STATIC_DIR)\n",
        "    copytree(gs_project_dir, project_dir)\n",
        "   \n",
        "  res = {\n",
        "        'status': 'OK'\n",
        "  }\n",
        "  \n",
        "  return jsonify(res), 201\n",
        "\n",
        "@app.route('/saveToUSB', methods=[\"POST\"])\n",
        "def saveToUSB():\n",
        "  rjson = request.json\n",
        "  project_dir = os.path.join(STATIC_DIR, rjson[\"projectName\"])\n",
        "  gs_project_dir = os.path.join('/content/drive/My Drive/', rjson[\"projectName\"])\n",
        "\n",
        "  print(rjson[\"projectName\"])\n",
        "  print(gs_project_dir)\n",
        "  print(os.path.isdir(gs_project_dir))\n",
        "  if(os.path.isdir(gs_project_dir) == True):\n",
        "    print(\"directory exists\")\n",
        "    os.system('rm -rf ' + gs_project_dir)\n",
        "    os.system('cp -rf ' + project_dir + ' ' + GOOGLE_DRIVE)\n",
        "  else:\n",
        "    os.system('cp -rf ' + project_dir + ' ' + GOOGLE_DRIVE)\n",
        "  res = {\n",
        "        'status': 'OK'\n",
        "  }\n",
        "  \n",
        "  return jsonify(res), 201\n",
        "  \n",
        "@app.route('/saveXML', methods=[\"POST\"])\n",
        "def saveXML():\n",
        "  res = {\n",
        "        'status': 'OK'\n",
        "  }\n",
        "  \n",
        "  return jsonify(res), 201\n",
        "\n",
        "@app.route('/gsGetProjects', methods=['POST'])\n",
        "def gsGetProjects():\n",
        "    dds = []\n",
        "    for x in os.listdir('/content/drive/My Drive/'):\n",
        "      if os.path.isdir(os.path.join('/content/drive/My Drive/', x)):\n",
        "        for fname in os.listdir(os.path.join('/content/drive/My Drive/', x)):\n",
        "            if fname.endswith('.xml'):\n",
        "                # do stuff on the file\n",
        "                print(x)\n",
        "                dds.append(x)\n",
        "                break\n",
        "            else:\n",
        "            # do stuff if a file .true doesn't exist.\n",
        "                pass\n",
        "            \n",
        "    res = {\n",
        "        'status': 'OK',\n",
        "        'projects': dds\n",
        "    }\n",
        "  \n",
        "    return jsonify(res), 201\n",
        "\n",
        "\n",
        "@app.route('/writeXml', methods=['POST'])\n",
        "def writeXml():\n",
        "    rjson = request.json\n",
        "    filename = os.path.join(STATIC_DIR,  rjson[\"path\"] , \"images\", rjson[\"filename\"])\n",
        "    xml_data = rjson[\"data\"]\n",
        "    print(xml_data)\n",
        "    root = ET.fromstring(xml_data)\n",
        "    print(filename)\n",
        "\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(filename)\n",
        "\n",
        "    res = {\n",
        "        'status': 'OK'\n",
        "    }\n",
        "  \n",
        "    return jsonify(res), 201\n",
        "\n",
        "@app.route('/checkXmlFile', methods=['POST'])\n",
        "def checkXmlFile():\n",
        "    rjson = request.json\n",
        "    filename = os.path.join(STATIC_DIR, rjson[\"projectpath\"] , \"images\", rjson[\"filename\"])\n",
        "    print(\"Chencking xml\")\n",
        "    print(filename)\n",
        "    if(os.path.isfile(filename) == True):\n",
        "        tree    = ET.parse(filename)\n",
        "        xml = tree.getroot()\n",
        "        xmlstr = ET.tostring(xml, encoding='utf8', method='xml')\n",
        "        \n",
        "        #parsed = xmljson.badgerfish.data(xml)\n",
        "        parsed = xmltodict.parse(xmlstr)\n",
        "        print(xml)\n",
        "        res = {\n",
        "            'status': 'OK',\n",
        "            'data' : parsed\n",
        "        }\n",
        "    else:\n",
        "        res = {\n",
        "            'status': 'FAIL'\n",
        "        }\n",
        "    return jsonify(res), 201\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/imclassAnotaion', methods=['POST'])\n",
        "def imclassAnotaion():\n",
        "    req = request.json\n",
        "    dirname = os.path.join(STATIC_DIR, req[\"projectpath\"], \"imgclass\", req[\"dirname\"])\n",
        "    try:\n",
        "        os.mkdir(dirname)\n",
        "        res = {\n",
        "            'status': 'OK',\n",
        "        }\n",
        "    except OSError:\n",
        "        res = {\n",
        "            'status': 'FAIL',\n",
        "        }\n",
        "    return jsonify(res), 201\n",
        "\n",
        "\n",
        "@app.route('/getAnotaions', methods=['POST'])\n",
        "def getAnotaions():\n",
        "    req = request.json\n",
        "    dirname = os.path.join(STATIC_DIR, req[\"projectpath\"], \"imgclass\")\n",
        "    try:\n",
        "        dds = list_paths(dirname)\n",
        "        print(\"get dds\")\n",
        "        print(dds)\n",
        "        res = {\n",
        "            'classes': dds,\n",
        "            'status': 'OK'\n",
        "        }\n",
        "    except OSError:\n",
        "        res = {\n",
        "            'status': 'FAIL',\n",
        "        }\n",
        "    return jsonify(res), 201\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/addClass', methods=['POST'])\n",
        "def addClass():\n",
        "    content = request.json\n",
        "    filePath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"imclass.json\")\n",
        "    print(filePath)\n",
        "    res = {\n",
        "        'status': 'OK'\n",
        "    }\n",
        "    return jsonify(res), 201\n",
        "\n",
        "\n",
        "@app.route(\"/upload\", methods=['POST'])\n",
        "def upload_file():\n",
        "    content = request.json\n",
        "    filePath = os.path.join(STATIC_DIR, content[\"projectpath\"])\n",
        "    tfpath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\")\n",
        "\n",
        "    print(tfpath)\n",
        "    if(os.path.isdir(tfpath) == True):\n",
        "      print(\"Directory exist!!!!!!\")\n",
        "      try:  \n",
        "        shutil.rmtree(tfpath)  \n",
        "        print(\"% s removed successfully\" % tfpath)  \n",
        "      except OSError as error:  \n",
        "        print(error)  \n",
        "        print(\"File path can not be removed\") \n",
        "    trainPath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"images\", \"train\")\n",
        "    testPath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"images\", \"test\")\n",
        "    dataPath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"data\")\n",
        "    pretrainPath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"pretrained_model\")\n",
        "    trainedPath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"trained\")\n",
        "    \n",
        "    os.mkdir(os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\"))\n",
        "    os.mkdir(os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"images\"))\n",
        "    os.mkdir(testPath) \n",
        "    os.mkdir(trainPath) \n",
        "    os.mkdir(dataPath) \n",
        "    os.mkdir(pretrainPath)\n",
        "    os.mkdir(trainedPath)\n",
        "    imagesPath = os.path.join(STATIC_DIR, content[\"projectpath\"],'images') + '/*.* ' + os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"images\", \"train\")\n",
        "    subprocess.call('cp ' + imagesPath, shell=True)\n",
        "\n",
        "    xml_files = glob.glob(trainPath + '/*.xml')\n",
        "    xml_files_chossen = random.choices(xml_files, k=4)\n",
        "    png_files = [os.path.splitext(x)[0]+'.png' for x in xml_files_chossen]\n",
        "\n",
        "\n",
        "    for xml_f in xml_files_chossen:\n",
        "      f_name = os.path.basename(xml_f)\n",
        "      #shutil.move(os.path.join(trainPath, f_name), os.path.join(testPath, f_name))\n",
        "      subprocess.call('mv ' + os.path.join(trainPath, f_name) + ' ' + os.path.join(testPath, f_name), shell=True)\n",
        "\n",
        "    for xml_f in png_files:\n",
        "      f_name = os.path.basename(xml_f)\n",
        "      #shutil.move(os.path.join(trainPath, f_name), os.path.join(testPath, f_name))\n",
        "      subprocess.call('mv ' + os.path.join(trainPath, f_name) + ' ' + os.path.join(testPath, f_name), shell=True)\n",
        "    \n",
        "    #print('ls -Q ' + trainPath + ' | head -10 | xargs -i mv ' + trainPath + '{} ' + testPath)\n",
        "    #os.system('ls -Q ' + trainPath + ' | head -10 | xargs -i mv ' + trainPath + '/{} ' + testPath)\n",
        "\n",
        "    #Copy pretained model\n",
        "    copytree(os.path.join(TEMPLATE_DIR, MODEL),  os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"pretrained_model\"))\n",
        "\n",
        "    xml_df=xml_to_csv(trainPath)\n",
        "    xml_df.to_csv(os.path.join(dataPath, 'train_labels.csv'), index=None)\n",
        "    xml_df=xml_to_csv(testPath)\n",
        "    xml_df.to_csv(os.path.join(dataPath, 'test_labels.csv'), index=None)\n",
        "\n",
        "\n",
        "    #log(\"Getting number of labels...\")\n",
        "    csv_file_train = pd.read_csv(os.path.join(dataPath, 'train_labels.csv')) \n",
        "    column_val_list_train = csv_file_train['class']\n",
        "    all_labels_train = set(column_val_list_train) \n",
        "\n",
        "    csv_file_test = pd.read_csv(os.path.join(dataPath, 'test_labels.csv')) \n",
        "    column_val_list_test = csv_file_test['class']\n",
        "    all_labels_test = set(column_val_list_test) \n",
        "\n",
        "    all_labels = all_labels_train.union(all_labels_test)\n",
        "    #log(\"All labels = \"+ str(all_labels))\n",
        "\n",
        "    s_labels = list(all_labels)\n",
        "    #log(\"All classes = \"+ str(s_labels))\n",
        "\n",
        "    NUM_OF_CLASS = len(s_labels)\n",
        "    print(NUM_OF_CLASS)\n",
        "\n",
        "    #log(\"Number of classes = \"+ str(NUM_OF_CLASS))\n",
        "    #log(\"Number of training steps = \"+ str(NUM_OF_TRAINING_STEP))\n",
        "\n",
        "    def convert_classes(classes, start=1):\n",
        "      msg = StringIntLabelMap()\n",
        "      label_dict = {}\n",
        "      for id, name in enumerate(classes, start=start):\n",
        "        msg.item.append(StringIntLabelMapItem(id=id, name=name))\n",
        "        label_dict[name] = id\n",
        "\n",
        "      text = str(text_format.MessageToBytes(msg, as_utf8=True), 'utf-8')\n",
        "      return text, label_dict\n",
        "\n",
        "    txt, label_dict = convert_classes(s_labels)\n",
        "    print(txt)\n",
        "\n",
        "    with open(os.path.join(dataPath, 'object-detection.pbtxt'), 'w') as f:\n",
        "      f.write(txt)\n",
        "\n",
        "    gen_tfrecord(image_dir=trainPath, csv_input=os.path.join(dataPath, 'train_labels.csv'), output_path=os.path.join(dataPath, 'train.record'), label_dict=label_dict)\n",
        "    \n",
        "    gen_tfrecord(image_dir=testPath , csv_input=os.path.join(dataPath, 'test_labels.csv'), output_path=os.path.join(dataPath, 'test.record'), label_dict=label_dict)\n",
        " \n",
        "    os.environ['CONFIG_FILE'] = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"pipeline_mobilenet_v1_ssd_retrain_last_few_layers_edited.config\")\n",
        "    filenameOut = os.environ['CONFIG_FILE']\n",
        "\n",
        "    #filename = '/root/tfdata/pipeline_mobilenet_v1_ssd_retrain_last_few_layers.config'\n",
        "    if(MODEL_NAME == \"V1\"):\n",
        "      filename = '/root/TPU-MobilenetSSD/colaboratory/gpu/pipeline_mobilenet_v1_ssd_retrain_last_few_layers.config'\n",
        "    else:\n",
        "      filename = '/root/TPU-MobilenetSSD/colaboratory/gpu/pipeline_mobilenet_v2_ssd_retrain_last_few_layers.config'\n",
        "    #filename = '/root/tfdata/pipeline_mobilenet_v2_ssdlite_retrain_last_few_layers.config'\n",
        "\n",
        "    filenameOut = os.environ['CONFIG_FILE']\n",
        "\n",
        "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig() \n",
        "\n",
        "    \n",
        "\n",
        "    with tf.gfile.GFile(filename, \"r\") as f:                                                                                                                                                                                                                     \n",
        "      proto_str = f.read()                                                                                                                                                                                                                                          \n",
        "      text_format.Merge(proto_str, pipeline_config)\n",
        "\n",
        "\n",
        "    pipeline_config.model.ssd.num_classes = NUM_OF_CLASS\n",
        "    pipeline_config.train_config.num_steps = NUM_OF_TRAINING_STEP\n",
        "    pipeline_config.train_config.fine_tune_checkpoint=os.path.join(pretrainPath, \"model.ckpt\")\n",
        "    pipeline_config.train_input_reader.label_map_path=os.path.join(dataPath, 'object-detection.pbtxt')\n",
        "    pipeline_config.train_input_reader.tf_record_input_reader.input_path[0]=os.path.join(dataPath, \"train.record\") \n",
        "    pipeline_config.eval_input_reader[0].label_map_path=os.path.join(dataPath, 'object-detection.pbtxt')\n",
        "    pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0]=os.path.join(dataPath,\"test.record\") \n",
        "    #pipeline_config.train_config.batch_size=8\n",
        "\n",
        "    config_text = text_format.MessageToString(pipeline_config)   \n",
        "    print(config_text)                                                                                                                                                                                                    \n",
        "    with tf.gfile.Open(filenameOut, \"wb\") as f:                                                                                                                                                                                                                       \n",
        "      f.write(config_text)   \n",
        "    cmd_train = 'python ~/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path=' + filenameOut + ' \\\n",
        "    --model_dir='+trainedPath + ' \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps='+str(NUM_OF_TRAINING_STEP)+' \\\n",
        "    --num_eval_steps=0'\n",
        "\n",
        "    print(cmd_train)\n",
        "    run(cmd_train)\n",
        "\n",
        "    outputFineTuned = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"fine_tuned_model\")\n",
        "\n",
        "\n",
        "    #log(\"[Training Thread] Exporting Check point to Saved model...\")\n",
        "    trained_checkpoint_prefix = trainedPath+'/model.ckpt-'+str(NUM_OF_TRAINING_STEP)\n",
        "    export_dir = os.path.join(trainedPath, '0')\n",
        "\n",
        "    graph = tf.Graph()\n",
        "    with tf.compat.v1.Session(graph=graph) as sess:\n",
        "      # Restore from checkpoint\n",
        "      loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\n",
        "      loader.restore(sess, trained_checkpoint_prefix)\n",
        "\n",
        "      # Export checkpoint to SavedModel\n",
        "      builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\n",
        "      builder.add_meta_graph_and_variables(sess,\n",
        "                                          [tf.saved_model.TRAINING, tf.saved_model.SERVING],\n",
        "                                          strip_default_attrs=True)\n",
        "      builder.save()  \n",
        "\n",
        "\n",
        "    lst = os.listdir(trainedPath)\n",
        "    lf = filter(lambda k: 'model.ckpt-' in k, lst)\n",
        "    last_model = sorted(lf)[-1].replace('.meta', '')\n",
        "    #log(last_model)\n",
        "    os.environ['last_model']=last_model\n",
        "\n",
        "   \n",
        "\n",
        "    run('python /root/models/research/object_detection/export_inference_graph.py \\\n",
        "      --input_type=image_tensor \\\n",
        "      --pipeline_config_path=$CONFIG_FILE \\\n",
        "      --output_directory=' + outputFineTuned + ' \\\n",
        "      --trained_checkpoint_prefix=' + trainedPath +'/$last_model')\n",
        "    PATH_TO_FROZEN_GRAPH = os.path.join(outputFineTuned, 'frozen_inference_graph.pb')\n",
        "    with detection_graph.as_default():\n",
        "      od_graph_def = tf.GraphDef()\n",
        "      with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "    res = {\n",
        "        'status': 'OK'\n",
        "    }\n",
        "    return jsonify(res), 201\n",
        "\n",
        " \n",
        "@app.route(\"/detect\", methods=['POST'])\n",
        "def detect():\n",
        "\n",
        "    content = request.json\n",
        "    outputFineTuned = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"fine_tuned_model\")\n",
        "\n",
        "    PATH_TO_FROZEN_GRAPH = os.path.join(outputFineTuned, 'frozen_inference_graph.pb')\n",
        "    with detection_graph.as_default():\n",
        "      od_graph_def = tf.GraphDef()\n",
        "      with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "    content = request.json\n",
        "    testPath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"images\", \"test\")\n",
        "    PATH_TO_LABELS = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"data\", \"object-detection.pbtxt\")\n",
        "    #testPath = os.path.join(STATIC_DIR, content[\"projectpath\"], \"images\")\n",
        "    png_files = []\n",
        "    for file in os.listdir(testPath):\n",
        "        if file.endswith(\".png\"):\n",
        "            png_files.append(file)\n",
        "    \n",
        "    #image_path = os.path.join(testPath, content[\"filename\"])\n",
        "    image_path = os.path.join(testPath, random.choice(png_files))\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert(\"RGB\")\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # convert numpy array to PIL Image\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "    \n",
        "    img = Image.fromarray(image_np.astype('uint8'))\n",
        "\n",
        "    # create file-object in memory\n",
        "    file_object = io.BytesIO()\n",
        "\n",
        "    # write PNG in file-object\n",
        "    img.save(file_object, 'PNG')\n",
        "\n",
        "    # move to beginning of file so `send_file()` it will read from start    \n",
        "    file_object.seek(0)\n",
        "\n",
        "    encoded_img = encodebytes(file_object.getvalue()).decode('ascii')\n",
        "    res =  { 'Status' : 'Success', 'ImageBytes': encoded_img}\n",
        "\n",
        "    return jsonify(res), 201\n",
        "    #return send_file(file_object, mimetype='image/PNG')\n",
        "\n",
        "\n",
        "\n",
        "@app.route(\"/loadFrozenGraph\", methods=['POST'])\n",
        "def loadFrozenGraph():\n",
        "   \n",
        "    content = request.json\n",
        "    outputFineTuned = os.path.join(STATIC_DIR, content[\"projectpath\"], \"tfdata\", \"fine_tuned_model\")\n",
        "\n",
        "    PATH_TO_FROZEN_GRAPH = os.path.join(outputFineTuned, 'frozen_inference_graph.pb')\n",
        "    with detection_graph.as_default():\n",
        "      od_graph_def = tf.GraphDef()\n",
        "      with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        " \n",
        "\n",
        "\n",
        "    res = {\n",
        "      'status': 'OK'\n",
        "    }\n",
        "    return jsonify(res), 201 \n",
        "\n",
        "@app.route(\"/multiple-files\", methods=[\"POST\"])\n",
        "def upload_multiple_files():\n",
        "    print(\"Save multiple files\")\n",
        "    uploaded_files = request.files\n",
        "    print(request.form)\n",
        "    for key in request.files:\n",
        "        print(key)\n",
        "    print(\"********\")\n",
        "    ii = request.files.values()\n",
        "    while True:\n",
        "        try:\n",
        "            # get the next item\n",
        "            element = next(ii)\n",
        "            print(element.filename)\n",
        "            filePath = os.path.join(STATIC_DIR, request.form['projectpath'], 'images', element.filename) \n",
        "            im = Image.open(BytesIO(element.read()))\n",
        "            basename = os.path.splitext(os.path.basename(element.filename))[0] + '.png'\n",
        "\n",
        "            basewidth = 640\n",
        "            \n",
        "            wpercent = (basewidth/float(im.size[0]))\n",
        "            hsize = int((float(im.size[1])*float(wpercent)))\n",
        "            new_image = im.resize((basewidth,hsize), Image.ANTIALIAS)\n",
        "            #new_image = im.resize((640,480))\n",
        "            filePath2 = os.path.join(STATIC_DIR, request.form['projectpath'], 'images', basename)\n",
        "            new_image.save(filePath2)\n",
        "            \n",
        "            # do something with element\n",
        "        except StopIteration:\n",
        "            # if StopIteration is raised, break from loop\n",
        "            break\n",
        "  \n",
        "    res = {\n",
        "        'status': 'OK'\n",
        "    }\n",
        "    return jsonify(res), 201\n",
        "\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "#app.run()\n",
        "#app.run(host='0.0.0.0', port=5000, ssl_context='adhoc')\n",
        "#context = ('/content/kbai/server.crt', '/content/kbai/server.key')\n",
        "context = ('cert.pem', 'key.pem')\n",
        "#app.run(host='0.0.0.0', port=5000, ssl_context=context)\n",
        "app.run(host='0.0.0.0', port=5000)\n",
        "#app.run(port=80)\n",
        "\n",
        "#import threading\n",
        "#threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':5000}).start()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-p0qTOdMnVU"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(3000)\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xel74nL_6iq_"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIFS2DNObvhb"
      },
      "source": [
        "import IPython.display\n",
        "\n",
        "def display(port, height):\n",
        "    shell = \"\"\"\n",
        "        (async () => {\n",
        "            const url = await google.colab.kernel.proxyPort(%PORT%, {\"cache\": true});\n",
        "            const iframe = document.createElement('iframe');\n",
        "            iframe.src = url;\n",
        "            iframe.setAttribute('width', '100%');\n",
        "            iframe.setAttribute('height', '%HEIGHT%');\n",
        "            iframe.setAttribute('frameborder', 0);\n",
        "            document.body.appendChild(iframe);\n",
        "        })();\n",
        "    \"\"\"\n",
        "    replacements = [\n",
        "        (\"%PORT%\", \"%d\" % port),\n",
        "        (\"%HEIGHT%\", \"%d\" % height),\n",
        "    ]\n",
        "    for (k, v) in replacements:\n",
        "        shell = shell.replace(k, v)\n",
        "\n",
        "    script = IPython.display.Javascript(shell)\n",
        "    IPython.display.display(script)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M68EcKR4EA3G"
      },
      "source": [
        "display(5000, 800)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}